Prompt Definitivo Ottimizzato per Novapad (Rust) — Massima Stealth 2026

Stai lavorando sulla codebase Rust "Novapad", un lettore RSS.  
Obiettivo: Rendere il fetching HTTP degli articoli indistinguibile da un vero browser Chrome su Windows, con massima stealth contro sistemi anti-bot moderni (Cloudflare, Akamai, DataDome), gestione intelligente dei paywall soft, e robustezza contro rate-limiting.[1[2

*

Vincoli Non Negoziabili

1. Non refactorizzare codice non correlato alla pipeline fetch articoli.
2. Prima rileva l'implementazione esistente (client HTTP, headers, retry logic). Aggiungi solo ciò che manca.
3. Mantieni comportamento stabile per utenti esistenti (zero breaking changes).
4. Compatibilità Windows obbligatoria.
5. Logging di debug per feature anti-bot (MAI loggare cookie/secrets).

*

Target Primario

Pipeline: RSS Feed → URL articolo → fetch HTML come browser → estrazione testo

Focus: layer di rete HTTP con emulazione browser perfetta.

*

Step 0 — Discovery nel Repo

Analizza e documenta:
Dove viene creato il client HTTP attuale
Se esiste client condiviso o istanze multiple
Cookie store esistente
Headers configurati
Gestione errori HTTP (403/429/503)

*

Soluzione Unica Ottimale: wreq con Fine-Tuning Avanzato

Dipendenze (Cargo.toml)

toml
[dependencies
wreq = { version = "3", features = ["cookies", "brotli", "gzip", "zstd", "http2" }
wreq-util = "3"
tokio = { version = "1", features = ["full" }
rand = "0.8"
log = "0.4"
url = "2"


*

A) Client HTTP con Emulazione Chrome Perfezionata

Crea un solo client globale con configurazione avanzata HTTP/2 per eludere fingerprinting sofisticato:[3[1

rust
use std::time::Duration;
use wreq::Client;
use wreq_util::Emulation;

pub fn build_stealth_client() -> Client {
Client::builder()
// Emulazione Chrome più recente
.emulation(Emulation::Chrome130)

// HTTP/2 fine-tuning per Cloudflare/Akamai
.http2_initial_stream_window_size(6_291_456)      // Valore esatto Chrome
.http2_initial_connection_window_size(15_728_640) // Valore esatto Chrome
.http2_adaptive_window(true)                       // Anti-fingerprinting dinamico

// Cookie store con persistenza
.cookie_store(true)

// Timeout realistico
.timeout(Duration::from_secs(30))
.connect_timeout(Duration::from_secs(10))

// Connection pooling (simula browser che riusa connessioni)
.pool_max_idle_per_host(6)  // Chrome default

.build()
.expect("Failed to build stealth HTTP client");

log::debug!("Stealth HTTP client initialized: Chrome130 + HTTP/2 adaptive window");
client
}


Perché questi parametri sono critici:[3
http2_initial_stream_window_size e http2_initial_connection_window_size: valori esatti di Chrome, rilevati da Akamai/Cloudflare tramite analisi SETTINGS frame HTTP/2
http2_adaptive_window: evita pattern fissi di window update che identificano bot
pool_max_idle_per_host: simula il connection pooling reale di Chrome (6 connessioni idle per host)[4

*

B) Headers con Ordine Browser-Perfect e Referer Intelligente

Non sovrascrivere gli header gestiti dall'emulazione (User-Agent, sec-ch-ua-*, Accept-Encoding).[2
Aggiungi solo header context-specific:

rust
use url::Url;
use wreq::RequestBuilder;

fn build_rss_request(
client: &Client,
article_url: &str,
feed_url: Option<&str>,
) -> RequestBuilder {
let mut req = client.get(article_url);

// Referer strategy: mai vuoto, sempre dal feed origin
if let Some(feed) = feed_url {
if let Ok(parsed) = Url::parse(feed) {
let origin = parsed.origin().ascii_serialization();
req = req.header("Referer", origin);
}
}

// Accept-Language italiano (sovrascrivi solo questo)
req = req.header("Accept-Language", "it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7");

// Cache-Control realistico (browser real refresh)
req = req.header("Cache-Control", "max-age=0");

req
}


Nota critica: L'ordine degli header HTTP/2 è gestito automaticamente da wreq tramite pseudo-header :authority, :method, :path, :scheme in sequenza corretta.[5

*

C) Timing Umano a 3 Livelli (Anti-Pattern Detection)

I sistemi anti-bot del 2026 rilevano pattern temporali troppo regolari. Implementa timing umano su 3 livelli:[6[4

rust
use rand::Rng;
use tokio::time::{sleep, Duration};

// Livello 1: Delay tra articoli dello stesso feed (50-200ms)
async fn inter_article_delay() {
let delay_ms = rand::thread_rng().gen_range(50..=200);
sleep(Duration::from_millis(delay_ms)).await;
}

// Livello 2: Delay tra feed diversi (1-3s, simula switch tab)
async fn inter_feed_delay() {
let delay_ms = rand::thread_rng().gen_range(1000..=3000);
sleep(Duration::from_millis(delay_ms)).await;
}

// Livello 3: "Think time" casuale ogni N richieste (5-10s, simula lettura)
async fn occasional_think_time(request_count: u32) {
if request_count % 10 == 0 {  // Ogni 10 richieste
let delay_ms = rand::thread_rng().gen_range(5000..=10000);
log::debug!("Simulating user think time: {}ms", delay_ms);
sleep(Duration::from_millis(delay_ms)).await;
}
}


Pattern di utilizzo:
rust
let mut request_count = 0;
for article in feed.articles {
inter_article_delay().await;
fetch_article(&client, &article.url).await?;
request_count += 1;
occasional_think_time(request_count).await;
}
inter_feed_delay().await;  // Prima di passare al prossimo feed


Questo crea un profilo temporale umano realistico invece di richieste meccaniche.[4

*

D) Retry Logic Intelligente con Rispetto RFC 6585

rust
use wreq::{Response, StatusCode};

const MAX_RETRIES: u32 = 3;

async fn fetch_with_smart_retry(
client: &Client,
url: &str,
feed_url: Option<&str>,
) -> Result<Response, FetchError> {
let mut attempt = 0;

loop {
let req = build_rss_request(client, url, feed_url);

let response = match req.send().await {
Ok(r) => r,
Err(e) if e.is_timeout() => {
log::warn!("Timeout on attempt {}/{}", attempt + 1, MAX_RETRIES);
if attempt >= MAX_RETRIES {
return Err(FetchError::Timeout);
}
attempt += 1;
sleep(Duration::from_secs(2u64.pow(attempt))).await;
continue;
}
Err(e) => return Err(FetchError::Network(e.to_string())),
};

match response.status() {
StatusCode::TOO_MANY_REQUESTS | StatusCode::SERVICE_UNAVAILABLE => {
if attempt >= MAX_RETRIES {
return Err(FetchError::TooManyRetries);
}

// RFC 6585: leggi Retry-After header
let retry_secs = response.headers()
.get("Retry-After")
.and_then(|v| v.to_str().ok())
.and_then(|s| s.parse::<u64>().ok())
.unwrap_or_else(|| {
// Exponential backoff: 2^attempt
let backoff = 2u64.pow(attempt + 1);
log::debug!("No Retry-After header, using exponential backoff: {}s", backoff);
backoff
});

log::warn!(
"Rate limited ({}), retrying after {}s (attempt {}/{})",
response.status(), retry_secs, attempt + 1, MAX_RETRIES
);

sleep(Duration::from_secs(retry_secs)).await;
attempt += 1;
continue;
}
_ => return Ok(response),
}
}
}


*

E) Rilevamento Anti-Bot Multi-Layer (2026 Signatures)

Sistema di detection a 2 fasi: status code + body analysis:[7

rust
[derive(Debug)
pub enum FetchError {
RequiresBrowser { url: String, reason: String },
Network(String),
Timeout,
TooManyRetries,
}

async fn detect_antibot_protection(
url: &str,
response: Response,
) -> Result<String, FetchError> {
let status = response.status();

// Fase 1: Status code immediati
match status.as_u16() {
403 | 1020 => {  // 1020 = Cloudflare Access Denied custom code
log::warn!("Access forbidden ({}): {}", status, url);
return Err(FetchError::RequiresBrowser {
url: url.to_string(),
reason: format!("HTTP {}", status),
});
}
_ => {}
}

let body = response.text().await.map_err(|e| FetchError::Network(e.to_string()))?;
let lower = body.to_lowercase();

// Fase 2: Signature 2026 (pattern aggiornati)
let antibot_markers = [
("cloudflare", "just a moment"),
("cloudflare", "cf_chl_"),
("cloudflare", "ray id:"),
("cloudflare", "challenge-platform"),
("datadome", "datadome-protected"),
("perimetere_x", "px-captcha"),
("akamai", "access denied"),
("generic", "verify you are human"),
("generic", "enable javascript and cookies"),
];

for (system, signature) in antibot_markers {
if lower.contains(signature) {
log::warn!("Anti-bot detected: {} (signature: {})", system, signature);
return Err(FetchError::RequiresBrowser {
url: url.to_string(),
reason: format!("{} protection", system),
});
}
}

Ok(body)
}


UI Integration:
rust
match fetch_article_safe(&client, url).await {
Err(FetchError::RequiresBrowser { url, reason }) => {
show_browser_fallback_ui(&url, &reason);  // "Apri in Browser" button
}
Ok(html) => parse_and_display(html),
Err(e) => show_error(&e),
}


*

F) Connection Pooling Realistico

Il client già configurato con pool_max_idle_per_host(6) simula il comportamento di Chrome che riusa connessioni TCP/TLS allo stesso host. Questo:[1
Riduce overhead TLS handshake ripetuti
Crea pattern di traffico più simili a browser reale
Evita "troppe connessioni nuove" che sono un red flag per anti-bot[6

*

G) Logging Strutturato (Debug Mode)

rust
// Esempio output in modalità debug
log::debug!("Stealth client initialized: Chrome130, HTTP/2 adaptive, 6 conn/host");
log::info!("Fetching article: {} (from feed: {})", article_url, feed_url.unwrap_or("direct"));
log::debug!("Human delay: 127ms");
log::warn!("Rate limited (429), retry after 8s (attempt 2/3)");
log::error!("Anti-bot detected: cloudflare (signature: cf_chl_)");


Regola assoluta: MAI loggare cookie, token, o header di autenticazione.

*

Output Richiesto

1. Patch PR-ready con:
Client wreq centralizzato con HTTP/2 tuning
Timing umano a 3 livelli
Retry logic RFC-compliant
Detection anti-bot multi-signature
Integration points nel codice esistente

2. Documentazione tecnica che spiega:
Perché http2_adaptive_window evita Akamai detection[3
Come il timing a 3 livelli previene pattern analysis[4
Perché connection pooling (6 idle) simula Chrome[1
Differenza tra TLS fingerprinting (JA3) e HTTP/2 fingerprinting (SETTINGS frame)[2

3. Test su siti critici:
Cloudflare: https://nowsecure.nl (test site)
Paywall soft: Medium.com, NYTimes.com
Rate limiting: Reddit RSS feeds

4. Metriche di successo:
Tasso di successo fetch articoli >95%
Tempo medio fetch <2s
Zero false-positive "RequiresBrowser" su siti normali

*

Perché Questa è la Soluzione Migliore 2026

✅ HTTP/2 SETTINGS frame perfect match con Chrome reale[3
✅ Timing pattern umano a 3 livelli anti-analysis[4
✅ Connection pooling identico a browser[1
✅ Multi-signature detection aggiornata a sistemi 2026[7
✅ RFC 6585 compliant retry logic professionale  
✅ Zero overhead rispetto a browser headless (Puppeteer/Playwright)[6

Questa soluzione integra tutto ciò che serve in un'unica strategia coerente, testata e mantenuta dal crate wreq, senza compromessi o alternative parallele.[8[1

1
2
3
4
5
6
7
8
9
10